{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fasion_mnist_Practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMS2oxG8mMyU4/2Mk5tRK0B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jungmin0710/K-digital-PythonDeepLearning/blob/main/Fasion_mnist_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paQaXO1qNsTx"
      },
      "source": [
        "# Keras - Fasion MNIST\n",
        "1. Fasion_mnist_작성자이름.ipynb\n",
        "2. Image Data\n",
        "3. Categorical Classification\n",
        "4. Metric -> 'Accuracy', 'Overfitting','Callbacks'\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG5WTWS4Nmnh"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6SRD4p7OI7P"
      },
      "source": [
        "## 0.Import Tensorflow & Keras\n",
        "* import Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "E10HzXTZOIWE",
        "outputId": "13ef4fb1-0f12-4a53-d674-ef9f5e790947"
      },
      "source": [
        "import keras\n",
        "\n",
        "keras.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5-nOB20Oyvd"
      },
      "source": [
        "* import Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f9RnlK-5OOt_",
        "outputId": "606ca93e-69ac-430c-cefb-c44466adc067"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNqg6-pwO3aX"
      },
      "source": [
        "* GPU 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dUTINsTwO04J",
        "outputId": "d5b6f577-875c-42c5-bf2e-b21c39a0b8a1"
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzThUB2PPAoc"
      },
      "source": [
        "## 1.Fasion MNIST Data_Set Load & Review\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjpoHEzNYzcC"
      },
      "source": [
        "\n",
        "### 1)Load Fasion MNIST Data_Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou6bXRLFO4c1"
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcD19SmePTYV"
      },
      "source": [
        "* Train_Data Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAuQRZ_xPRVl",
        "outputId": "667b50c0-0bf5-45e5-a0a3-531ed24b1a79"
      },
      "source": [
        "print(len(X_train))\n",
        "print(X_train.shape)\n",
        "\n",
        "print(len(y_train))\n",
        "print(y_train[0:5])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n",
            "(60000, 28, 28)\n",
            "60000\n",
            "[9 0 0 3 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OFHQK03PdKJ"
      },
      "source": [
        "* Test_Data Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiIDZADCPcge",
        "outputId": "b2914e7e-6cd6-43be-e84e-14a0f720c852"
      },
      "source": [
        "print(len(X_test))\n",
        "print(X_test.shape)\n",
        "\n",
        "print(len(y_test))\n",
        "print(y_test[0:5])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "(10000, 28, 28)\n",
            "10000\n",
            "[9 2 1 1 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74frBYXSPnpu"
      },
      "source": [
        "### 2)Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "0Lqaj1m4PkYl",
        "outputId": "d387182d-1c88-40e5-d691-3b305e50098f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 어떤 이미지인지 확인하기(흑백)\n",
        "digit = X_train[0]\n",
        "plt.imshow(digit, cmap = 'gray')\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYebP22lPwFl",
        "outputId": "083c84c8-2896-4094-f660-f4a731e88124"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 2차원 행렬로 확인하기\n",
        "np.set_printoptions(linewidth = 150)\n",
        "\n",
        "print(X_train[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0   0   1   4   0   0   0   0   1   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62  54   0   0   0   1   3   4   0   0   3]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134 144 123  23   0   0   0   0  12  10   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178 107 156 161 109  64  23  77 130  72  15]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216 216 163 127 121 122 146 141  88 172  66]\n",
            " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229 223 223 215 213 164 127 123 196 229   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228 235 227 224 222 224 221 223 245 173   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198 180 212 210 211 213 223 220 243 202   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192 169 227 208 218 224 212 226 197 209  52]\n",
            " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203 198 221 215 213 222 220 245 119 167  56]\n",
            " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240 232 213 218 223 234 217 217 209  92   0]\n",
            " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219 222 221 216 223 229 215 218 255  77   0]\n",
            " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208 211 218 224 223 219 215 224 244 159   0]\n",
            " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230 224 234 176 188 250 248 233 238 215   0]\n",
            " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223 255 255 221 234 221 211 220 232 246   0]\n",
            " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221 188 154 191 210 204 209 222 228 225   0]\n",
            " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117 168 219 221 215 217 223 223 224 229  29]\n",
            " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245 239 223 218 212 209 222 220 221 230  67]\n",
            " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216 199 206 186 181 177 172 181 205 206 115]\n",
            " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191 195 191 198 192 176 156 167 177 210  92]\n",
            " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209 210 210 211 188 188 194 192 216 170   0]\n",
            " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179 182 182 181 176 166 168  99  58   0   0]\n",
            " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuVjntXrQLq0"
      },
      "source": [
        "## 2.Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE9sHF8fY4Tn"
      },
      "source": [
        "### 1)Reshape and Normalization\n",
        "* reshape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8SXXfwqP-ct",
        "outputId": "26a9ed06-13a3-428b-da24-6a84bd96a8ed"
      },
      "source": [
        "#3차원으로 구성된 데이터를 한줄로 쭉 펼쳐 2차원 형태로 만든다(가로로 길게 잘라 옆에붙인다)\n",
        "X_train = X_train.reshape((60000, 28 * 28))\n",
        "X_test = X_test.reshape((10000, 28 * 28))\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 784), (10000, 784))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORB7BhuDQ1T1"
      },
      "source": [
        "* Normaliztion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTmpimrvQz0-",
        "outputId": "f19d30ea-e9e0-41dc-ccba-8e7633a49f69"
      },
      "source": [
        "# model에 집어넣기 위해 데이터를 실수화 한 후 정규화시킨다\n",
        "X_train = X_train.astype(float) / 255\n",
        "X_test = X_test.astype(float) / 255\n",
        "\n",
        "print(X_train[2])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.         0.         0.         0.         0.         0.         0.         0.         0.08627451 0.4627451  0.09411765 0.\n",
            " 0.         0.         0.         0.         0.18823529 0.34509804 0.01960784 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.04705882 0.39215686 0.83137255\n",
            " 0.80392157 0.7254902  0.70196078 0.67843137 0.72941176 0.75686275 0.86666667 0.55686275 0.33333333 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.33333333 0.29803922 0.78039216 0.88235294 0.97254902 1.         0.93333333 0.88627451 0.61568627 0.26666667 0.31372549 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.35686275 0.27058824 0.35686275 0.78823529 0.85490196 0.88235294 0.81960784 0.61960784 0.23921569 0.36470588 0.28235294\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.30980392 0.34901961 0.23921569 0.23137255 0.34117647 0.42352941 0.29411765 0.21960784 0.29803922\n",
            " 0.38039216 0.28627451 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.29411765 0.34901961 0.31372549 0.31372549 0.2627451  0.24705882 0.28627451\n",
            " 0.3254902  0.31372549 0.37647059 0.28235294 0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.30196078 0.34509804 0.30196078 0.31372549 0.3254902\n",
            " 0.3254902  0.3254902  0.3254902  0.31764706 0.37254902 0.29803922 0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.34901961 0.37647059 0.31372549\n",
            " 0.3254902  0.31764706 0.32941176 0.33333333 0.33333333 0.33333333 0.38039216 0.32941176 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.36470588\n",
            " 0.38039216 0.31764706 0.33333333 0.32941176 0.33333333 0.34117647 0.34509804 0.32941176 0.38823529 0.34117647 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.37254902 0.34117647 0.32941176 0.34117647 0.34509804 0.33333333 0.34117647 0.34117647 0.32941176 0.36078431 0.34117647 0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.38039216 0.34117647 0.34117647 0.33333333 0.34509804 0.34117647 0.34117647 0.34117647 0.34509804 0.33333333\n",
            " 0.41960784 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.06666667 0.39215686 0.34509804 0.34117647 0.34117647 0.34509804 0.34117647 0.34117647 0.33333333\n",
            " 0.34901961 0.30196078 0.4627451  0.03137255 0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.03921569 0.36470588 0.34117647 0.34117647 0.34117647 0.34117647 0.34117647\n",
            " 0.34509804 0.34117647 0.34901961 0.31372549 0.40392157 0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.03529412 0.37647059 0.34117647 0.34117647 0.34117647\n",
            " 0.34117647 0.34117647 0.34509804 0.34117647 0.34509804 0.34117647 0.40392157 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.04705882 0.37647059 0.33333333\n",
            " 0.34117647 0.34117647 0.34117647 0.33333333 0.34117647 0.34117647 0.34509804 0.34901961 0.39215686 0.00784314 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.07843137\n",
            " 0.37254902 0.32941176 0.34509804 0.33333333 0.34117647 0.34509804 0.34509804 0.34509804 0.34901961 0.34509804 0.38823529 0.03137255 0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.08235294 0.37647059 0.33333333 0.34117647 0.33333333 0.34509804 0.34509804 0.34509804 0.34509804 0.34901961 0.34901961 0.38823529\n",
            " 0.03921569 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.09411765 0.37647059 0.33333333 0.34117647 0.33333333 0.34117647 0.34509804 0.34509804 0.34901961 0.34509804\n",
            " 0.35686275 0.4        0.05490196 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.09803922 0.36470588 0.32941176 0.34509804 0.34117647 0.34117647 0.34117647 0.34117647\n",
            " 0.34117647 0.34901961 0.35686275 0.40392157 0.11372549 0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.11764706 0.37254902 0.33333333 0.34509804 0.34509804 0.34117647\n",
            " 0.34117647 0.34117647 0.34117647 0.34901961 0.34509804 0.4        0.14509804 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.13333333 0.37647059 0.34509804 0.34117647\n",
            " 0.34117647 0.34117647 0.34117647 0.34117647 0.34117647 0.33333333 0.33333333 0.38039216 0.14901961 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.15686275 0.37647059\n",
            " 0.34117647 0.33333333 0.34117647 0.34117647 0.34117647 0.34117647 0.34117647 0.33333333 0.32941176 0.36078431 0.19215686 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.18039216 0.37254902 0.3254902  0.32941176 0.34117647 0.34117647 0.34117647 0.34117647 0.34117647 0.34117647 0.32941176 0.34117647 0.32941176\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.28235294 0.37254902 0.33333333 0.32941176 0.33333333 0.34509804 0.34117647 0.34117647 0.34901961 0.34117647 0.33333333\n",
            " 0.3254902  0.24705882 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.25098039 0.39215686 0.32941176 0.34117647 0.34509804 0.33333333 0.34509804 0.34509804 0.32941176\n",
            " 0.34117647 0.3254902  0.37254902 0.20784314 0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.03921569 0.4        0.39215686 0.35686275 0.35686275 0.34901961 0.33333333\n",
            " 0.32941176 0.32941176 0.34117647 0.42352941 0.41568627 0.05490196 0.         0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.03137255 0.28627451 0.36470588 0.40784314\n",
            " 0.41960784 0.40392157 0.40392157 0.41568627 0.4        0.29411765 0.03921569 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.00392157 0.         0.\n",
            " 0.         0.07058824 0.16470588 0.22352941 0.21960784 0.1254902  0.03137255 0.         0.         0.00392157 0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11zXwzXdRLbe"
      },
      "source": [
        "### 2)One Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znxqbytiRHM5",
        "outputId": "a95c6ff3-597e-415d-d275-8afae005f087"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "#결과값을 인코딩한 후 실수형으로 만든다(1.값이 존재하는 인덱스가 실제 y값에 해당)\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "print(y_train[:5])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt5NFg9bRhSy"
      },
      "source": [
        "## 3.MNIST Keras Modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bzt7os0IY7g1"
      },
      "source": [
        "### 1)Model Define"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wvXXCP7RalV"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "mnist = models.Sequential()\n",
        "mnist.add(layers.Dense(512, activation = 'relu', input_shape = (28 * 28,)))\n",
        "mnist.add(layers.Dense(256, activation = 'relu'))\n",
        "mnist.add(layers.Dense(10, activation = 'softmax'))  #다중분류에는 softmax / 이중분류는 sigmoid"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7-XCjLASX9e",
        "outputId": "dbd0261f-9408-4e3e-dba3-5dfba1373ff9"
      },
      "source": [
        "mnist.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 535,818\n",
            "Trainable params: 535,818\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or6nFMnCSd1_"
      },
      "source": [
        "### 2)Model Compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d4TE-v3SZ5x"
      },
      "source": [
        "mnist.compile(loss = 'categorical_crossentropy', #다중분류모델\n",
        "              optimizer = 'rmsprop',\n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cwH4DTgSoRK"
      },
      "source": [
        "### 3)Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGmDlftFSm8P",
        "outputId": "859d5f06-79ef-4645-8858-9a4855f53f1d"
      },
      "source": [
        "%%time\n",
        "\n",
        "Hist_mnist = mnist.fit(X_train, y_train,\n",
        "                       epochs = 100,\n",
        "                       batch_size = 128,\n",
        "                       validation_split = 0.2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "375/375 [==============================] - 3s 4ms/step - loss: 0.7794 - accuracy: 0.7239 - val_loss: 0.3989 - val_accuracy: 0.8545\n",
            "Epoch 2/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.4005 - accuracy: 0.8500 - val_loss: 0.3937 - val_accuracy: 0.8574\n",
            "Epoch 3/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.3408 - accuracy: 0.8746 - val_loss: 0.3309 - val_accuracy: 0.8831\n",
            "Epoch 4/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.3101 - accuracy: 0.8833 - val_loss: 0.3732 - val_accuracy: 0.8739\n",
            "Epoch 5/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2915 - accuracy: 0.8926 - val_loss: 0.3299 - val_accuracy: 0.8840\n",
            "Epoch 6/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2763 - accuracy: 0.8966 - val_loss: 0.3356 - val_accuracy: 0.8830\n",
            "Epoch 7/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2668 - accuracy: 0.9014 - val_loss: 0.3425 - val_accuracy: 0.8848\n",
            "Epoch 8/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2509 - accuracy: 0.9059 - val_loss: 0.3173 - val_accuracy: 0.8942\n",
            "Epoch 9/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2372 - accuracy: 0.9096 - val_loss: 0.3314 - val_accuracy: 0.8916\n",
            "Epoch 10/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2337 - accuracy: 0.9139 - val_loss: 0.3713 - val_accuracy: 0.8920\n",
            "Epoch 11/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2308 - accuracy: 0.9136 - val_loss: 0.3543 - val_accuracy: 0.8858\n",
            "Epoch 12/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2210 - accuracy: 0.9177 - val_loss: 0.3713 - val_accuracy: 0.8893\n",
            "Epoch 13/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2151 - accuracy: 0.9200 - val_loss: 0.3632 - val_accuracy: 0.8928\n",
            "Epoch 14/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2086 - accuracy: 0.9210 - val_loss: 0.4099 - val_accuracy: 0.8877\n",
            "Epoch 15/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.2010 - accuracy: 0.9247 - val_loss: 0.4085 - val_accuracy: 0.8901\n",
            "Epoch 16/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1920 - accuracy: 0.9269 - val_loss: 0.4201 - val_accuracy: 0.8873\n",
            "Epoch 17/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1931 - accuracy: 0.9272 - val_loss: 0.5387 - val_accuracy: 0.8809\n",
            "Epoch 18/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1913 - accuracy: 0.9290 - val_loss: 0.4665 - val_accuracy: 0.8817\n",
            "Epoch 19/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1840 - accuracy: 0.9324 - val_loss: 0.5210 - val_accuracy: 0.8888\n",
            "Epoch 20/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1836 - accuracy: 0.9323 - val_loss: 0.4620 - val_accuracy: 0.8822\n",
            "Epoch 21/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1800 - accuracy: 0.9328 - val_loss: 0.4607 - val_accuracy: 0.8910\n",
            "Epoch 22/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1769 - accuracy: 0.9360 - val_loss: 0.4563 - val_accuracy: 0.8936\n",
            "Epoch 23/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1732 - accuracy: 0.9355 - val_loss: 0.4780 - val_accuracy: 0.8918\n",
            "Epoch 24/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1656 - accuracy: 0.9387 - val_loss: 0.4860 - val_accuracy: 0.8893\n",
            "Epoch 25/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1659 - accuracy: 0.9383 - val_loss: 0.5246 - val_accuracy: 0.8918\n",
            "Epoch 26/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1672 - accuracy: 0.9385 - val_loss: 0.4907 - val_accuracy: 0.8983\n",
            "Epoch 27/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1609 - accuracy: 0.9409 - val_loss: 0.6040 - val_accuracy: 0.8747\n",
            "Epoch 28/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1550 - accuracy: 0.9423 - val_loss: 0.5024 - val_accuracy: 0.8972\n",
            "Epoch 29/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1514 - accuracy: 0.9445 - val_loss: 0.5295 - val_accuracy: 0.8949\n",
            "Epoch 30/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1501 - accuracy: 0.9441 - val_loss: 0.7023 - val_accuracy: 0.8725\n",
            "Epoch 31/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1555 - accuracy: 0.9444 - val_loss: 0.7108 - val_accuracy: 0.8889\n",
            "Epoch 32/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1565 - accuracy: 0.9458 - val_loss: 0.5654 - val_accuracy: 0.8946\n",
            "Epoch 33/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1445 - accuracy: 0.9470 - val_loss: 0.5643 - val_accuracy: 0.8955\n",
            "Epoch 34/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1475 - accuracy: 0.9465 - val_loss: 0.6228 - val_accuracy: 0.8922\n",
            "Epoch 35/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1428 - accuracy: 0.9488 - val_loss: 0.5817 - val_accuracy: 0.8923\n",
            "Epoch 36/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1435 - accuracy: 0.9486 - val_loss: 0.6143 - val_accuracy: 0.8890\n",
            "Epoch 37/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1367 - accuracy: 0.9499 - val_loss: 0.6056 - val_accuracy: 0.8982\n",
            "Epoch 38/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1395 - accuracy: 0.9503 - val_loss: 0.7691 - val_accuracy: 0.8847\n",
            "Epoch 39/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1406 - accuracy: 0.9515 - val_loss: 0.6502 - val_accuracy: 0.8910\n",
            "Epoch 40/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1405 - accuracy: 0.9516 - val_loss: 0.6451 - val_accuracy: 0.8932\n",
            "Epoch 41/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1244 - accuracy: 0.9540 - val_loss: 0.6743 - val_accuracy: 0.8923\n",
            "Epoch 42/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1362 - accuracy: 0.9529 - val_loss: 0.6888 - val_accuracy: 0.8900\n",
            "Epoch 43/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1358 - accuracy: 0.9529 - val_loss: 0.6910 - val_accuracy: 0.8941\n",
            "Epoch 44/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1268 - accuracy: 0.9548 - val_loss: 0.8146 - val_accuracy: 0.8868\n",
            "Epoch 45/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1229 - accuracy: 0.9566 - val_loss: 0.8016 - val_accuracy: 0.8876\n",
            "Epoch 46/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1290 - accuracy: 0.9543 - val_loss: 0.7975 - val_accuracy: 0.8898\n",
            "Epoch 47/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1225 - accuracy: 0.9569 - val_loss: 0.7739 - val_accuracy: 0.8923\n",
            "Epoch 48/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1228 - accuracy: 0.9566 - val_loss: 0.7727 - val_accuracy: 0.8917\n",
            "Epoch 49/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1145 - accuracy: 0.9580 - val_loss: 0.7800 - val_accuracy: 0.8936\n",
            "Epoch 50/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1129 - accuracy: 0.9599 - val_loss: 0.7646 - val_accuracy: 0.8932\n",
            "Epoch 51/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1129 - accuracy: 0.9604 - val_loss: 0.8240 - val_accuracy: 0.8936\n",
            "Epoch 52/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1222 - accuracy: 0.9584 - val_loss: 0.8029 - val_accuracy: 0.8917\n",
            "Epoch 53/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1168 - accuracy: 0.9592 - val_loss: 0.9621 - val_accuracy: 0.8847\n",
            "Epoch 54/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1128 - accuracy: 0.9594 - val_loss: 0.8954 - val_accuracy: 0.8892\n",
            "Epoch 55/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1098 - accuracy: 0.9614 - val_loss: 0.8906 - val_accuracy: 0.8934\n",
            "Epoch 56/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1120 - accuracy: 0.9627 - val_loss: 0.9145 - val_accuracy: 0.8903\n",
            "Epoch 57/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1216 - accuracy: 0.9616 - val_loss: 0.8837 - val_accuracy: 0.8947\n",
            "Epoch 58/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1114 - accuracy: 0.9611 - val_loss: 0.9262 - val_accuracy: 0.8921\n",
            "Epoch 59/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1122 - accuracy: 0.9605 - val_loss: 1.0269 - val_accuracy: 0.8953\n",
            "Epoch 60/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1091 - accuracy: 0.9632 - val_loss: 1.1613 - val_accuracy: 0.8692\n",
            "Epoch 61/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1151 - accuracy: 0.9614 - val_loss: 1.0705 - val_accuracy: 0.8863\n",
            "Epoch 62/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1109 - accuracy: 0.9627 - val_loss: 0.9329 - val_accuracy: 0.8911\n",
            "Epoch 63/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1114 - accuracy: 0.9629 - val_loss: 0.9794 - val_accuracy: 0.8956\n",
            "Epoch 64/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1033 - accuracy: 0.9635 - val_loss: 0.9398 - val_accuracy: 0.8918\n",
            "Epoch 65/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1044 - accuracy: 0.9661 - val_loss: 1.0557 - val_accuracy: 0.8967\n",
            "Epoch 66/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1071 - accuracy: 0.9643 - val_loss: 1.1098 - val_accuracy: 0.8935\n",
            "Epoch 67/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1063 - accuracy: 0.9660 - val_loss: 1.0303 - val_accuracy: 0.8946\n",
            "Epoch 68/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0988 - accuracy: 0.9660 - val_loss: 0.9400 - val_accuracy: 0.8923\n",
            "Epoch 69/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1062 - accuracy: 0.9648 - val_loss: 1.0767 - val_accuracy: 0.8905\n",
            "Epoch 70/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1061 - accuracy: 0.9659 - val_loss: 1.1443 - val_accuracy: 0.8906\n",
            "Epoch 71/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0954 - accuracy: 0.9679 - val_loss: 1.0202 - val_accuracy: 0.8938\n",
            "Epoch 72/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0995 - accuracy: 0.9671 - val_loss: 1.0680 - val_accuracy: 0.8963\n",
            "Epoch 73/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1016 - accuracy: 0.9666 - val_loss: 1.1504 - val_accuracy: 0.8937\n",
            "Epoch 74/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0930 - accuracy: 0.9678 - val_loss: 1.2921 - val_accuracy: 0.8932\n",
            "Epoch 75/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0957 - accuracy: 0.9682 - val_loss: 1.2025 - val_accuracy: 0.8890\n",
            "Epoch 76/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0916 - accuracy: 0.9699 - val_loss: 1.0830 - val_accuracy: 0.8909\n",
            "Epoch 77/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0918 - accuracy: 0.9689 - val_loss: 1.2162 - val_accuracy: 0.8935\n",
            "Epoch 78/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0901 - accuracy: 0.9687 - val_loss: 1.2605 - val_accuracy: 0.8943\n",
            "Epoch 79/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1012 - accuracy: 0.9682 - val_loss: 1.2806 - val_accuracy: 0.8896\n",
            "Epoch 80/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0914 - accuracy: 0.9708 - val_loss: 1.4082 - val_accuracy: 0.8845\n",
            "Epoch 81/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0905 - accuracy: 0.9696 - val_loss: 1.2545 - val_accuracy: 0.8889\n",
            "Epoch 82/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0903 - accuracy: 0.9716 - val_loss: 1.4358 - val_accuracy: 0.8907\n",
            "Epoch 83/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0956 - accuracy: 0.9688 - val_loss: 1.2749 - val_accuracy: 0.8918\n",
            "Epoch 84/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0950 - accuracy: 0.9715 - val_loss: 1.4021 - val_accuracy: 0.8918\n",
            "Epoch 85/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0988 - accuracy: 0.9704 - val_loss: 1.5727 - val_accuracy: 0.8904\n",
            "Epoch 86/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0988 - accuracy: 0.9717 - val_loss: 1.0834 - val_accuracy: 0.8930\n",
            "Epoch 87/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0907 - accuracy: 0.9711 - val_loss: 1.2988 - val_accuracy: 0.8905\n",
            "Epoch 88/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0789 - accuracy: 0.9727 - val_loss: 1.3295 - val_accuracy: 0.8928\n",
            "Epoch 89/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0944 - accuracy: 0.9729 - val_loss: 1.2755 - val_accuracy: 0.8973\n",
            "Epoch 90/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0841 - accuracy: 0.9724 - val_loss: 1.4454 - val_accuracy: 0.8891\n",
            "Epoch 91/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0849 - accuracy: 0.9717 - val_loss: 1.3775 - val_accuracy: 0.8819\n",
            "Epoch 92/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0910 - accuracy: 0.9708 - val_loss: 1.3802 - val_accuracy: 0.8842\n",
            "Epoch 93/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0807 - accuracy: 0.9731 - val_loss: 1.4904 - val_accuracy: 0.8924\n",
            "Epoch 94/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0888 - accuracy: 0.9712 - val_loss: 1.4364 - val_accuracy: 0.8901\n",
            "Epoch 95/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0866 - accuracy: 0.9735 - val_loss: 1.3979 - val_accuracy: 0.8920\n",
            "Epoch 96/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0890 - accuracy: 0.9736 - val_loss: 1.4073 - val_accuracy: 0.8907\n",
            "Epoch 97/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0795 - accuracy: 0.9735 - val_loss: 1.9991 - val_accuracy: 0.8597\n",
            "Epoch 98/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0857 - accuracy: 0.9731 - val_loss: 1.4563 - val_accuracy: 0.8912\n",
            "Epoch 99/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0785 - accuracy: 0.9749 - val_loss: 1.5131 - val_accuracy: 0.8838\n",
            "Epoch 100/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.0804 - accuracy: 0.9744 - val_loss: 1.7171 - val_accuracy: 0.8891\n",
            "CPU times: user 2min 8s, sys: 19.2 s, total: 2min 27s\n",
            "Wall time: 1min 58s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey_65zgWS2kf"
      },
      "source": [
        "### 4)Result Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "kZ-QSkwUSwDi",
        "outputId": "a156b774-6d22-418f-ea9c-134fb4940731"
      },
      "source": [
        "epochs = range(1, len(Hist_mnist.history['accuracy']) + 1)\n",
        "\n",
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(epochs, Hist_mnist.history['accuracy'])\n",
        "plt.plot(epochs, Hist_mnist.history['val_accuracy'])\n",
        "plt.title('Training & Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Training accuracy', 'Validation accuracy'])\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "#Test Data의 정확도가 90퍼센트를 넘지 못해 모델성능이 좋지는 않다\n",
        "#과적합 문제가 발생했다고 볼 수 있다"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LUyXRL6TGYT"
      },
      "source": [
        "### 5)Model Evaluate\n",
        "* Loss & Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biV44_rbTEUd",
        "outputId": "86101b21-894b-4b40-b870-f16c83eb0e2c"
      },
      "source": [
        "loss, accuracy = mnist.evaluate(X_test, y_test)\n",
        "\n",
        "print('Loss = {:.5f}'.format(loss))\n",
        "print('Accuracy = {:.5f}'.format(accuracy))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 1.9119 - accuracy: 0.8837\n",
            "Loss = 1.91188\n",
            "Accuracy = 0.88370\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17XJcyy7TMpj"
      },
      "source": [
        "### 6)Model Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2rVly6ZTRBf",
        "outputId": "4181c509-5581-4285-98d7-0473b0a60ed8"
      },
      "source": [
        "#첫번째 데이터 Probability\n",
        "np.set_printoptions(suppress=True, precision=9)\n",
        "\n",
        "print(mnist.predict(X_test[:1,:]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rOrYrWlTfmL",
        "outputId": "85e06404-6bb2-47a3-93bf-d85eac0b4552"
      },
      "source": [
        "#첫번째 데이터 Class\n",
        "print(mnist.predict_classes(X_test[:1, :]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTEV2CV5UkZQ"
      },
      "source": [
        "## 4.MNIST Keras Modeling - Overfitting Issue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU3Gr2PFY_kk"
      },
      "source": [
        "### 1)Model Define"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM_ynmNTUkZa"
      },
      "source": [
        "#from keras import regularizers\n",
        "\n",
        "#최적의 성능 찾기 : 여러조합 시도하기 \n",
        "mnist_new = models.Sequential()\n",
        "mnist_new.add(layers.Dense(512, input_shape = (28 * 28,)))\n",
        "mnist_new.add(layers.Dropout(0.5))\n",
        "mnist_new.add(layers.BatchNormalization())\n",
        "mnist_new.add(layers.Activation('relu'))\n",
        "mnist_new.add(layers.Dense(256))\n",
        "mnist_new.add(layers.Dropout(0.2))\n",
        "mnist_new.add(layers.BatchNormalization())\n",
        "mnist_new.add(layers.Activation('relu'))\n",
        "mnist_new.add(layers.Dense(64, activation = 'relu'))\n",
        "mnist_new.add(layers.Dropout(0.2))\n",
        "mnist_new.add(layers.BatchNormalization())\n",
        "mnist_new.add(layers.Activation('relu'))\n",
        "mnist_new.add(layers.Dense(10, activation = 'softmax'))"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChSfWRnjUkZd",
        "outputId": "75f305cf-ad66-4426-fc77-a0f6a161d6fd"
      },
      "source": [
        "mnist_new.summary()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_19 (Dense)             (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 553,674\n",
            "Trainable params: 552,010\n",
            "Non-trainable params: 1,664\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aCF4HsTUkZg"
      },
      "source": [
        "### 2)Model Compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIWqMZmcUkZj"
      },
      "source": [
        "mnist_new.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = 'rmsprop', #adam보다 나은듯\n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AJo3IG8UkZo"
      },
      "source": [
        "### 3)Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFQfT0MuUkZo",
        "outputId": "38794be3-d01b-4701-daa7-803da33dc024"
      },
      "source": [
        "%%time\n",
        "\n",
        "Hist_mnist_new = mnist_new.fit(X_train, y_train,\n",
        "                       epochs = 100,\n",
        "                       batch_size = 128,\n",
        "                       validation_split = 0.2)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "375/375 [==============================] - 3s 5ms/step - loss: 0.8801 - accuracy: 0.7046 - val_loss: 0.5011 - val_accuracy: 0.8157\n",
            "Epoch 2/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.4667 - accuracy: 0.8359 - val_loss: 0.3859 - val_accuracy: 0.8596\n",
            "Epoch 3/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.4206 - accuracy: 0.8490 - val_loss: 0.3708 - val_accuracy: 0.8648\n",
            "Epoch 4/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.4026 - accuracy: 0.8573 - val_loss: 0.3633 - val_accuracy: 0.8621\n",
            "Epoch 5/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.3819 - accuracy: 0.8633 - val_loss: 0.3584 - val_accuracy: 0.8687\n",
            "Epoch 6/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.3615 - accuracy: 0.8665 - val_loss: 0.3688 - val_accuracy: 0.8717\n",
            "Epoch 7/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.3582 - accuracy: 0.8718 - val_loss: 0.3440 - val_accuracy: 0.8785\n",
            "Epoch 8/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.3504 - accuracy: 0.8754 - val_loss: 0.3504 - val_accuracy: 0.8701\n",
            "Epoch 9/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.3404 - accuracy: 0.8755 - val_loss: 0.3618 - val_accuracy: 0.8618\n",
            "Epoch 10/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.3331 - accuracy: 0.8795 - val_loss: 0.3234 - val_accuracy: 0.8839\n",
            "Epoch 11/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.3241 - accuracy: 0.8816 - val_loss: 0.3253 - val_accuracy: 0.8824\n",
            "Epoch 12/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.3163 - accuracy: 0.8817 - val_loss: 0.3341 - val_accuracy: 0.8761\n",
            "Epoch 13/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.3081 - accuracy: 0.8878 - val_loss: 0.3520 - val_accuracy: 0.8734\n",
            "Epoch 14/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.3096 - accuracy: 0.8884 - val_loss: 0.3257 - val_accuracy: 0.8828\n",
            "Epoch 15/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.3016 - accuracy: 0.8898 - val_loss: 0.3388 - val_accuracy: 0.8762\n",
            "Epoch 16/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2932 - accuracy: 0.8918 - val_loss: 0.3333 - val_accuracy: 0.8801\n",
            "Epoch 17/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2917 - accuracy: 0.8919 - val_loss: 0.3223 - val_accuracy: 0.8852\n",
            "Epoch 18/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2917 - accuracy: 0.8940 - val_loss: 0.3167 - val_accuracy: 0.8871\n",
            "Epoch 19/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2838 - accuracy: 0.8954 - val_loss: 0.3192 - val_accuracy: 0.8881\n",
            "Epoch 20/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2816 - accuracy: 0.8997 - val_loss: 0.3205 - val_accuracy: 0.8839\n",
            "Epoch 21/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2792 - accuracy: 0.8990 - val_loss: 0.3311 - val_accuracy: 0.8788\n",
            "Epoch 22/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2668 - accuracy: 0.9022 - val_loss: 0.3056 - val_accuracy: 0.8911\n",
            "Epoch 23/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2702 - accuracy: 0.9016 - val_loss: 0.3166 - val_accuracy: 0.8889\n",
            "Epoch 24/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2694 - accuracy: 0.9019 - val_loss: 0.3185 - val_accuracy: 0.8878\n",
            "Epoch 25/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2660 - accuracy: 0.9042 - val_loss: 0.3147 - val_accuracy: 0.8847\n",
            "Epoch 26/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2588 - accuracy: 0.9068 - val_loss: 0.3107 - val_accuracy: 0.8880\n",
            "Epoch 27/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2593 - accuracy: 0.9063 - val_loss: 0.3308 - val_accuracy: 0.8855\n",
            "Epoch 28/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2538 - accuracy: 0.9047 - val_loss: 0.3044 - val_accuracy: 0.8896\n",
            "Epoch 29/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2553 - accuracy: 0.9068 - val_loss: 0.3051 - val_accuracy: 0.8912\n",
            "Epoch 30/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2519 - accuracy: 0.9061 - val_loss: 0.3210 - val_accuracy: 0.8873\n",
            "Epoch 31/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.2644 - accuracy: 0.9027 - val_loss: 0.3158 - val_accuracy: 0.8860\n",
            "Epoch 32/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2523 - accuracy: 0.9066 - val_loss: 0.3001 - val_accuracy: 0.8924\n",
            "Epoch 33/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2474 - accuracy: 0.9118 - val_loss: 0.3104 - val_accuracy: 0.8911\n",
            "Epoch 34/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2451 - accuracy: 0.9108 - val_loss: 0.3266 - val_accuracy: 0.8833\n",
            "Epoch 35/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2421 - accuracy: 0.9100 - val_loss: 0.3035 - val_accuracy: 0.8910\n",
            "Epoch 36/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2393 - accuracy: 0.9128 - val_loss: 0.3211 - val_accuracy: 0.8824\n",
            "Epoch 37/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2352 - accuracy: 0.9158 - val_loss: 0.3128 - val_accuracy: 0.8889\n",
            "Epoch 38/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2390 - accuracy: 0.9151 - val_loss: 0.3080 - val_accuracy: 0.8907\n",
            "Epoch 39/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2370 - accuracy: 0.9135 - val_loss: 0.3144 - val_accuracy: 0.8907\n",
            "Epoch 40/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2378 - accuracy: 0.9129 - val_loss: 0.3048 - val_accuracy: 0.8915\n",
            "Epoch 41/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2338 - accuracy: 0.9163 - val_loss: 0.3181 - val_accuracy: 0.8858\n",
            "Epoch 42/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2311 - accuracy: 0.9143 - val_loss: 0.3090 - val_accuracy: 0.8917\n",
            "Epoch 43/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2242 - accuracy: 0.9161 - val_loss: 0.3150 - val_accuracy: 0.8888\n",
            "Epoch 44/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2210 - accuracy: 0.9196 - val_loss: 0.3036 - val_accuracy: 0.8923\n",
            "Epoch 45/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2272 - accuracy: 0.9170 - val_loss: 0.2955 - val_accuracy: 0.8961\n",
            "Epoch 46/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2231 - accuracy: 0.9196 - val_loss: 0.3102 - val_accuracy: 0.8913\n",
            "Epoch 47/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2235 - accuracy: 0.9166 - val_loss: 0.3174 - val_accuracy: 0.8917\n",
            "Epoch 48/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2228 - accuracy: 0.9178 - val_loss: 0.3216 - val_accuracy: 0.8868\n",
            "Epoch 49/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2148 - accuracy: 0.9222 - val_loss: 0.3271 - val_accuracy: 0.8861\n",
            "Epoch 50/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2157 - accuracy: 0.9211 - val_loss: 0.3179 - val_accuracy: 0.8877\n",
            "Epoch 51/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2190 - accuracy: 0.9207 - val_loss: 0.3054 - val_accuracy: 0.8910\n",
            "Epoch 52/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2191 - accuracy: 0.9208 - val_loss: 0.3065 - val_accuracy: 0.8933\n",
            "Epoch 53/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2117 - accuracy: 0.9236 - val_loss: 0.3156 - val_accuracy: 0.8931\n",
            "Epoch 54/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2135 - accuracy: 0.9239 - val_loss: 0.3121 - val_accuracy: 0.8919\n",
            "Epoch 55/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2159 - accuracy: 0.9228 - val_loss: 0.3048 - val_accuracy: 0.8982\n",
            "Epoch 56/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2103 - accuracy: 0.9230 - val_loss: 0.3218 - val_accuracy: 0.8875\n",
            "Epoch 57/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2047 - accuracy: 0.9246 - val_loss: 0.3121 - val_accuracy: 0.8943\n",
            "Epoch 58/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2050 - accuracy: 0.9247 - val_loss: 0.3024 - val_accuracy: 0.8940\n",
            "Epoch 59/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1996 - accuracy: 0.9269 - val_loss: 0.3085 - val_accuracy: 0.8922\n",
            "Epoch 60/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2027 - accuracy: 0.9259 - val_loss: 0.3123 - val_accuracy: 0.8899\n",
            "Epoch 61/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2085 - accuracy: 0.9251 - val_loss: 0.3062 - val_accuracy: 0.8948\n",
            "Epoch 62/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.2012 - accuracy: 0.9258 - val_loss: 0.3225 - val_accuracy: 0.8861\n",
            "Epoch 63/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1963 - accuracy: 0.9272 - val_loss: 0.3255 - val_accuracy: 0.8888\n",
            "Epoch 64/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1949 - accuracy: 0.9279 - val_loss: 0.3172 - val_accuracy: 0.8911\n",
            "Epoch 65/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1937 - accuracy: 0.9298 - val_loss: 0.3162 - val_accuracy: 0.8943\n",
            "Epoch 66/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1946 - accuracy: 0.9292 - val_loss: 0.3178 - val_accuracy: 0.8918\n",
            "Epoch 67/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1935 - accuracy: 0.9301 - val_loss: 0.3160 - val_accuracy: 0.8895\n",
            "Epoch 68/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1868 - accuracy: 0.9317 - val_loss: 0.3166 - val_accuracy: 0.8912\n",
            "Epoch 69/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1980 - accuracy: 0.9287 - val_loss: 0.3160 - val_accuracy: 0.8921\n",
            "Epoch 70/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1942 - accuracy: 0.9288 - val_loss: 0.3208 - val_accuracy: 0.8901\n",
            "Epoch 71/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1928 - accuracy: 0.9293 - val_loss: 0.3182 - val_accuracy: 0.8891\n",
            "Epoch 72/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1910 - accuracy: 0.9303 - val_loss: 0.3169 - val_accuracy: 0.8877\n",
            "Epoch 73/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1904 - accuracy: 0.9321 - val_loss: 0.3151 - val_accuracy: 0.8895\n",
            "Epoch 74/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1897 - accuracy: 0.9303 - val_loss: 0.3084 - val_accuracy: 0.8946\n",
            "Epoch 75/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1897 - accuracy: 0.9313 - val_loss: 0.3291 - val_accuracy: 0.8852\n",
            "Epoch 76/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1870 - accuracy: 0.9308 - val_loss: 0.3163 - val_accuracy: 0.8878\n",
            "Epoch 77/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1885 - accuracy: 0.9311 - val_loss: 0.3098 - val_accuracy: 0.8913\n",
            "Epoch 78/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1822 - accuracy: 0.9334 - val_loss: 0.3233 - val_accuracy: 0.8873\n",
            "Epoch 79/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1808 - accuracy: 0.9351 - val_loss: 0.3181 - val_accuracy: 0.8865\n",
            "Epoch 80/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1799 - accuracy: 0.9346 - val_loss: 0.3248 - val_accuracy: 0.8923\n",
            "Epoch 81/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1784 - accuracy: 0.9345 - val_loss: 0.3177 - val_accuracy: 0.8890\n",
            "Epoch 82/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1838 - accuracy: 0.9340 - val_loss: 0.3105 - val_accuracy: 0.8942\n",
            "Epoch 83/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1814 - accuracy: 0.9344 - val_loss: 0.3049 - val_accuracy: 0.8965\n",
            "Epoch 84/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1780 - accuracy: 0.9377 - val_loss: 0.3138 - val_accuracy: 0.8932\n",
            "Epoch 85/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1774 - accuracy: 0.9354 - val_loss: 0.3097 - val_accuracy: 0.8902\n",
            "Epoch 86/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1746 - accuracy: 0.9360 - val_loss: 0.3287 - val_accuracy: 0.8919\n",
            "Epoch 87/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1716 - accuracy: 0.9365 - val_loss: 0.3139 - val_accuracy: 0.8946\n",
            "Epoch 88/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1754 - accuracy: 0.9372 - val_loss: 0.3152 - val_accuracy: 0.8944\n",
            "Epoch 89/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1729 - accuracy: 0.9378 - val_loss: 0.3309 - val_accuracy: 0.8867\n",
            "Epoch 90/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1716 - accuracy: 0.9377 - val_loss: 0.3247 - val_accuracy: 0.8879\n",
            "Epoch 91/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1709 - accuracy: 0.9373 - val_loss: 0.3406 - val_accuracy: 0.8835\n",
            "Epoch 92/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1769 - accuracy: 0.9355 - val_loss: 0.3371 - val_accuracy: 0.8835\n",
            "Epoch 93/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1691 - accuracy: 0.9396 - val_loss: 0.3218 - val_accuracy: 0.8898\n",
            "Epoch 94/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1716 - accuracy: 0.9375 - val_loss: 0.3229 - val_accuracy: 0.8854\n",
            "Epoch 95/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1676 - accuracy: 0.9394 - val_loss: 0.3101 - val_accuracy: 0.8915\n",
            "Epoch 96/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1731 - accuracy: 0.9394 - val_loss: 0.3240 - val_accuracy: 0.8891\n",
            "Epoch 97/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1662 - accuracy: 0.9400 - val_loss: 0.3250 - val_accuracy: 0.8913\n",
            "Epoch 98/100\n",
            "375/375 [==============================] - 2s 5ms/step - loss: 0.1685 - accuracy: 0.9386 - val_loss: 0.3323 - val_accuracy: 0.8896\n",
            "Epoch 99/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1665 - accuracy: 0.9401 - val_loss: 0.3171 - val_accuracy: 0.8938\n",
            "Epoch 100/100\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1689 - accuracy: 0.9382 - val_loss: 0.3140 - val_accuracy: 0.8908\n",
            "CPU times: user 2min 59s, sys: 18.7 s, total: 3min 18s\n",
            "Wall time: 2min 44s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neis8hUhUkZp"
      },
      "source": [
        "### 4)Result Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "HzYAdz6iUkZq",
        "outputId": "c3f34bf2-8349-4f60-801e-536763fd60f2"
      },
      "source": [
        "epochs = range(1, len(Hist_mnist_new.history['loss']) + 1)\n",
        "\n",
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(epochs, Hist_mnist_new.history['accuracy'])\n",
        "plt.plot(epochs, Hist_mnist_new.history['val_accuracy'])\n",
        "plt.title('Training & Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Training accuracy', 'Validation accuracy'])\n",
        "plt.grid()\n",
        "plt.show()\n",
        "##88 = > 89로 이전보다 향상됨"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggzCBqbnUkZr"
      },
      "source": [
        "### 5)Model Evaluate\n",
        "* Loss & Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPtWQLTBUkZt",
        "outputId": "f55c377c-f3e0-49a0-f71c-d555391eb053"
      },
      "source": [
        "loss, accuracy = mnist_new.evaluate(X_test, y_test)\n",
        "\n",
        "print('Loss = {:.5f}'.format(loss))\n",
        "print('Accuracy = {:.5f}'.format(accuracy))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3348 - accuracy: 0.8905\n",
            "Loss = 0.33478\n",
            "Accuracy = 0.89050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8hWuSWPUkZx"
      },
      "source": [
        "### 6)Model Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRdSZm8OUkZz",
        "outputId": "17ecefab-c6af-480b-de08-ef1321b0d4b4"
      },
      "source": [
        "#첫번째 데이터 Probability\n",
        "np.set_printoptions(suppress=True, precision=9)\n",
        "\n",
        "print(mnist_new.predict(X_test[:1,:]))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.000016554 0.000000016 0.000001582 0.000000103 0.00000039  0.013910547 0.000007116 0.02038856  0.000136902 0.96553826 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUlp4trVUkZ0",
        "outputId": "f1b23b0f-81d3-43aa-f4c3-90c97a606850"
      },
      "source": [
        "#첫번째 데이터 Class\n",
        "print(mnist_new.predict_classes(X_test[:1, :]))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4izFQKMaHCv"
      },
      "source": [
        "## 4. Ealry Stopping "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBZK4zXeaMIg"
      },
      "source": [
        "### 1)Keras Session Clear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENZ-37nkaN49"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "K.clear_session()"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbIeDeBSaOsl"
      },
      "source": [
        "### 2)Model Define & Compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBNY0kzxaTHA"
      },
      "source": [
        "#define - 위에서 찾은 최적모델\n",
        "mnist_fin = models.Sequential(name = 'EarlyStopping')\n",
        "mnist_fin.add(layers.Dense(256, input_shape = (28 * 28,)))\n",
        "mnist_fin.add(layers.Dropout(0.5))\n",
        "mnist_fin.add(layers.BatchNormalization())\n",
        "mnist_fin.add(layers.Activation('relu'))\n",
        "mnist_fin.add(layers.Dense(64))\n",
        "mnist_fin.add(layers.Dropout(0.2))\n",
        "mnist_fin.add(layers.BatchNormalization())\n",
        "mnist_fin.add(layers.Activation('relu'))\n",
        "mnist_fin.add(layers.Dense(10, activation = 'softmax'))\n",
        "\n",
        "#compile\n",
        "mnist_fin.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = 'rmsprop',\n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaDMh60ub94J"
      },
      "source": [
        "### 3)Ealry Stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3SD4li9b__c"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(monitor = 'accuracy',\n",
        "                   mode = 'max',\n",
        "                   patience = 20,\n",
        "                   verbose = 1)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0086F6fci8o"
      },
      "source": [
        "### 4)ModelCheckpoint()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51yc2rAjchn0"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "mc = ModelCheckpoint('best_mnist.h5',\n",
        "                     monitor = 'accuracy',\n",
        "                     mode = 'max',\n",
        "                     save_best_only = True,\n",
        "                     verbose = 1)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBu0I3-4czIM"
      },
      "source": [
        "### 5)Model Fit with callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukxy51BQc23A",
        "outputId": "f76ae4b6-982e-48b5-f922-7d66b794abad"
      },
      "source": [
        "%%time\n",
        "\n",
        "Hist_mnist_fin = mnist_fin.fit(X_train, y_train,\n",
        "                         epochs = 300,\n",
        "                         batch_size = 128,\n",
        "                         callbacks = [es, mc],\n",
        "                         verbose = 1)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3315 - accuracy: 0.8787\n",
            "\n",
            "Epoch 00001: accuracy improved from -inf to 0.87865, saving model to best_mnist.h5\n",
            "Epoch 2/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3271 - accuracy: 0.8831\n",
            "\n",
            "Epoch 00002: accuracy improved from 0.87865 to 0.88312, saving model to best_mnist.h5\n",
            "Epoch 3/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3203 - accuracy: 0.8833\n",
            "\n",
            "Epoch 00003: accuracy improved from 0.88312 to 0.88330, saving model to best_mnist.h5\n",
            "Epoch 4/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3182 - accuracy: 0.8854\n",
            "\n",
            "Epoch 00004: accuracy improved from 0.88330 to 0.88540, saving model to best_mnist.h5\n",
            "Epoch 5/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3132 - accuracy: 0.8866\n",
            "\n",
            "Epoch 00005: accuracy improved from 0.88540 to 0.88657, saving model to best_mnist.h5\n",
            "Epoch 6/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3090 - accuracy: 0.8882\n",
            "\n",
            "Epoch 00006: accuracy improved from 0.88657 to 0.88818, saving model to best_mnist.h5\n",
            "Epoch 7/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3070 - accuracy: 0.8882\n",
            "\n",
            "Epoch 00007: accuracy did not improve from 0.88818\n",
            "Epoch 8/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3043 - accuracy: 0.8894\n",
            "\n",
            "Epoch 00008: accuracy improved from 0.88818 to 0.88938, saving model to best_mnist.h5\n",
            "Epoch 9/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3000 - accuracy: 0.8897\n",
            "\n",
            "Epoch 00009: accuracy improved from 0.88938 to 0.88970, saving model to best_mnist.h5\n",
            "Epoch 10/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2993 - accuracy: 0.8915\n",
            "\n",
            "Epoch 00010: accuracy improved from 0.88970 to 0.89147, saving model to best_mnist.h5\n",
            "Epoch 11/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2970 - accuracy: 0.8922\n",
            "\n",
            "Epoch 00011: accuracy improved from 0.89147 to 0.89217, saving model to best_mnist.h5\n",
            "Epoch 12/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2924 - accuracy: 0.8931\n",
            "\n",
            "Epoch 00012: accuracy improved from 0.89217 to 0.89307, saving model to best_mnist.h5\n",
            "Epoch 13/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2896 - accuracy: 0.8942\n",
            "\n",
            "Epoch 00013: accuracy improved from 0.89307 to 0.89425, saving model to best_mnist.h5\n",
            "Epoch 14/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2886 - accuracy: 0.8949\n",
            "\n",
            "Epoch 00014: accuracy improved from 0.89425 to 0.89493, saving model to best_mnist.h5\n",
            "Epoch 15/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2872 - accuracy: 0.8963\n",
            "\n",
            "Epoch 00015: accuracy improved from 0.89493 to 0.89630, saving model to best_mnist.h5\n",
            "Epoch 16/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2854 - accuracy: 0.8958\n",
            "\n",
            "Epoch 00016: accuracy did not improve from 0.89630\n",
            "Epoch 17/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2838 - accuracy: 0.8961\n",
            "\n",
            "Epoch 00017: accuracy did not improve from 0.89630\n",
            "Epoch 18/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2831 - accuracy: 0.8970\n",
            "\n",
            "Epoch 00018: accuracy improved from 0.89630 to 0.89703, saving model to best_mnist.h5\n",
            "Epoch 19/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2798 - accuracy: 0.8978\n",
            "\n",
            "Epoch 00019: accuracy improved from 0.89703 to 0.89783, saving model to best_mnist.h5\n",
            "Epoch 20/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2761 - accuracy: 0.9008\n",
            "\n",
            "Epoch 00020: accuracy improved from 0.89783 to 0.90080, saving model to best_mnist.h5\n",
            "Epoch 21/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2740 - accuracy: 0.9011\n",
            "\n",
            "Epoch 00021: accuracy improved from 0.90080 to 0.90113, saving model to best_mnist.h5\n",
            "Epoch 22/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2719 - accuracy: 0.9015\n",
            "\n",
            "Epoch 00022: accuracy improved from 0.90113 to 0.90153, saving model to best_mnist.h5\n",
            "Epoch 23/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2704 - accuracy: 0.9033\n",
            "\n",
            "Epoch 00023: accuracy improved from 0.90153 to 0.90333, saving model to best_mnist.h5\n",
            "Epoch 24/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2725 - accuracy: 0.9015\n",
            "\n",
            "Epoch 00024: accuracy did not improve from 0.90333\n",
            "Epoch 25/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2639 - accuracy: 0.9039\n",
            "\n",
            "Epoch 00025: accuracy improved from 0.90333 to 0.90392, saving model to best_mnist.h5\n",
            "Epoch 26/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2656 - accuracy: 0.9028\n",
            "\n",
            "Epoch 00026: accuracy did not improve from 0.90392\n",
            "Epoch 27/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2659 - accuracy: 0.9032\n",
            "\n",
            "Epoch 00027: accuracy did not improve from 0.90392\n",
            "Epoch 28/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2648 - accuracy: 0.9049\n",
            "\n",
            "Epoch 00028: accuracy improved from 0.90392 to 0.90492, saving model to best_mnist.h5\n",
            "Epoch 29/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2599 - accuracy: 0.9069\n",
            "\n",
            "Epoch 00029: accuracy improved from 0.90492 to 0.90695, saving model to best_mnist.h5\n",
            "Epoch 30/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2595 - accuracy: 0.9056\n",
            "\n",
            "Epoch 00030: accuracy did not improve from 0.90695\n",
            "Epoch 31/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2572 - accuracy: 0.9063\n",
            "\n",
            "Epoch 00031: accuracy did not improve from 0.90695\n",
            "Epoch 32/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2592 - accuracy: 0.9063\n",
            "\n",
            "Epoch 00032: accuracy did not improve from 0.90695\n",
            "Epoch 33/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2569 - accuracy: 0.9067\n",
            "\n",
            "Epoch 00033: accuracy did not improve from 0.90695\n",
            "Epoch 34/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2536 - accuracy: 0.9073\n",
            "\n",
            "Epoch 00034: accuracy improved from 0.90695 to 0.90732, saving model to best_mnist.h5\n",
            "Epoch 35/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2523 - accuracy: 0.9091\n",
            "\n",
            "Epoch 00035: accuracy improved from 0.90732 to 0.90908, saving model to best_mnist.h5\n",
            "Epoch 36/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2523 - accuracy: 0.9078\n",
            "\n",
            "Epoch 00036: accuracy did not improve from 0.90908\n",
            "Epoch 37/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2507 - accuracy: 0.9088\n",
            "\n",
            "Epoch 00037: accuracy did not improve from 0.90908\n",
            "Epoch 38/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2495 - accuracy: 0.9106\n",
            "\n",
            "Epoch 00038: accuracy improved from 0.90908 to 0.91057, saving model to best_mnist.h5\n",
            "Epoch 39/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2511 - accuracy: 0.9087\n",
            "\n",
            "Epoch 00039: accuracy did not improve from 0.91057\n",
            "Epoch 40/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2464 - accuracy: 0.9100\n",
            "\n",
            "Epoch 00040: accuracy did not improve from 0.91057\n",
            "Epoch 41/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2460 - accuracy: 0.9107\n",
            "\n",
            "Epoch 00041: accuracy improved from 0.91057 to 0.91075, saving model to best_mnist.h5\n",
            "Epoch 42/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2430 - accuracy: 0.9116\n",
            "\n",
            "Epoch 00042: accuracy improved from 0.91075 to 0.91162, saving model to best_mnist.h5\n",
            "Epoch 43/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2433 - accuracy: 0.9128\n",
            "\n",
            "Epoch 00043: accuracy improved from 0.91162 to 0.91282, saving model to best_mnist.h5\n",
            "Epoch 44/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2421 - accuracy: 0.9113\n",
            "\n",
            "Epoch 00044: accuracy did not improve from 0.91282\n",
            "Epoch 45/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2439 - accuracy: 0.9119\n",
            "\n",
            "Epoch 00045: accuracy did not improve from 0.91282\n",
            "Epoch 46/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2417 - accuracy: 0.9125\n",
            "\n",
            "Epoch 00046: accuracy did not improve from 0.91282\n",
            "Epoch 47/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2394 - accuracy: 0.9146\n",
            "\n",
            "Epoch 00047: accuracy improved from 0.91282 to 0.91458, saving model to best_mnist.h5\n",
            "Epoch 48/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2409 - accuracy: 0.9131\n",
            "\n",
            "Epoch 00048: accuracy did not improve from 0.91458\n",
            "Epoch 49/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2389 - accuracy: 0.9133\n",
            "\n",
            "Epoch 00049: accuracy did not improve from 0.91458\n",
            "Epoch 50/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2355 - accuracy: 0.9155\n",
            "\n",
            "Epoch 00050: accuracy improved from 0.91458 to 0.91552, saving model to best_mnist.h5\n",
            "Epoch 51/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2353 - accuracy: 0.9149\n",
            "\n",
            "Epoch 00051: accuracy did not improve from 0.91552\n",
            "Epoch 52/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2347 - accuracy: 0.9146\n",
            "\n",
            "Epoch 00052: accuracy did not improve from 0.91552\n",
            "Epoch 53/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2349 - accuracy: 0.9158\n",
            "\n",
            "Epoch 00053: accuracy improved from 0.91552 to 0.91580, saving model to best_mnist.h5\n",
            "Epoch 54/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2334 - accuracy: 0.9147\n",
            "\n",
            "Epoch 00054: accuracy did not improve from 0.91580\n",
            "Epoch 55/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2303 - accuracy: 0.9170\n",
            "\n",
            "Epoch 00055: accuracy improved from 0.91580 to 0.91703, saving model to best_mnist.h5\n",
            "Epoch 56/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2317 - accuracy: 0.9168\n",
            "\n",
            "Epoch 00056: accuracy did not improve from 0.91703\n",
            "Epoch 57/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2292 - accuracy: 0.9176\n",
            "\n",
            "Epoch 00057: accuracy improved from 0.91703 to 0.91758, saving model to best_mnist.h5\n",
            "Epoch 58/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2291 - accuracy: 0.9167\n",
            "\n",
            "Epoch 00058: accuracy did not improve from 0.91758\n",
            "Epoch 59/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2278 - accuracy: 0.9176\n",
            "\n",
            "Epoch 00059: accuracy did not improve from 0.91758\n",
            "Epoch 60/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2293 - accuracy: 0.9176\n",
            "\n",
            "Epoch 00060: accuracy improved from 0.91758 to 0.91762, saving model to best_mnist.h5\n",
            "Epoch 61/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2271 - accuracy: 0.9176\n",
            "\n",
            "Epoch 00061: accuracy did not improve from 0.91762\n",
            "Epoch 62/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2256 - accuracy: 0.9182\n",
            "\n",
            "Epoch 00062: accuracy improved from 0.91762 to 0.91823, saving model to best_mnist.h5\n",
            "Epoch 63/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2259 - accuracy: 0.9198\n",
            "\n",
            "Epoch 00063: accuracy improved from 0.91823 to 0.91978, saving model to best_mnist.h5\n",
            "Epoch 64/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2220 - accuracy: 0.9183\n",
            "\n",
            "Epoch 00064: accuracy did not improve from 0.91978\n",
            "Epoch 65/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2247 - accuracy: 0.9201\n",
            "\n",
            "Epoch 00065: accuracy improved from 0.91978 to 0.92005, saving model to best_mnist.h5\n",
            "Epoch 66/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2201 - accuracy: 0.9199\n",
            "\n",
            "Epoch 00066: accuracy did not improve from 0.92005\n",
            "Epoch 67/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2232 - accuracy: 0.9192\n",
            "\n",
            "Epoch 00067: accuracy did not improve from 0.92005\n",
            "Epoch 68/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2185 - accuracy: 0.9211\n",
            "\n",
            "Epoch 00068: accuracy improved from 0.92005 to 0.92113, saving model to best_mnist.h5\n",
            "Epoch 69/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2183 - accuracy: 0.9209\n",
            "\n",
            "Epoch 00069: accuracy did not improve from 0.92113\n",
            "Epoch 70/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2215 - accuracy: 0.9205\n",
            "\n",
            "Epoch 00070: accuracy did not improve from 0.92113\n",
            "Epoch 71/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2171 - accuracy: 0.9215\n",
            "\n",
            "Epoch 00071: accuracy improved from 0.92113 to 0.92147, saving model to best_mnist.h5\n",
            "Epoch 72/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2160 - accuracy: 0.9221\n",
            "\n",
            "Epoch 00072: accuracy improved from 0.92147 to 0.92210, saving model to best_mnist.h5\n",
            "Epoch 73/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2189 - accuracy: 0.9202\n",
            "\n",
            "Epoch 00073: accuracy did not improve from 0.92210\n",
            "Epoch 74/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2176 - accuracy: 0.9207\n",
            "\n",
            "Epoch 00074: accuracy did not improve from 0.92210\n",
            "Epoch 75/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2156 - accuracy: 0.9216\n",
            "\n",
            "Epoch 00075: accuracy did not improve from 0.92210\n",
            "Epoch 76/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2147 - accuracy: 0.9227\n",
            "\n",
            "Epoch 00076: accuracy improved from 0.92210 to 0.92270, saving model to best_mnist.h5\n",
            "Epoch 77/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2119 - accuracy: 0.9243\n",
            "\n",
            "Epoch 00077: accuracy improved from 0.92270 to 0.92430, saving model to best_mnist.h5\n",
            "Epoch 78/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2136 - accuracy: 0.9235\n",
            "\n",
            "Epoch 00078: accuracy did not improve from 0.92430\n",
            "Epoch 79/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2096 - accuracy: 0.9244\n",
            "\n",
            "Epoch 00079: accuracy improved from 0.92430 to 0.92440, saving model to best_mnist.h5\n",
            "Epoch 80/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2120 - accuracy: 0.9239\n",
            "\n",
            "Epoch 00080: accuracy did not improve from 0.92440\n",
            "Epoch 81/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2145 - accuracy: 0.9236\n",
            "\n",
            "Epoch 00081: accuracy did not improve from 0.92440\n",
            "Epoch 82/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2131 - accuracy: 0.9220\n",
            "\n",
            "Epoch 00082: accuracy did not improve from 0.92440\n",
            "Epoch 83/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2080 - accuracy: 0.9245\n",
            "\n",
            "Epoch 00083: accuracy improved from 0.92440 to 0.92447, saving model to best_mnist.h5\n",
            "Epoch 84/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2112 - accuracy: 0.9251\n",
            "\n",
            "Epoch 00084: accuracy improved from 0.92447 to 0.92510, saving model to best_mnist.h5\n",
            "Epoch 85/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2079 - accuracy: 0.9243\n",
            "\n",
            "Epoch 00085: accuracy did not improve from 0.92510\n",
            "Epoch 86/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2105 - accuracy: 0.9242\n",
            "\n",
            "Epoch 00086: accuracy did not improve from 0.92510\n",
            "Epoch 87/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2048 - accuracy: 0.9260\n",
            "\n",
            "Epoch 00087: accuracy improved from 0.92510 to 0.92603, saving model to best_mnist.h5\n",
            "Epoch 88/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2084 - accuracy: 0.9241\n",
            "\n",
            "Epoch 00088: accuracy did not improve from 0.92603\n",
            "Epoch 89/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2068 - accuracy: 0.9255\n",
            "\n",
            "Epoch 00089: accuracy did not improve from 0.92603\n",
            "Epoch 90/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2070 - accuracy: 0.9268\n",
            "\n",
            "Epoch 00090: accuracy improved from 0.92603 to 0.92678, saving model to best_mnist.h5\n",
            "Epoch 91/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2063 - accuracy: 0.9256\n",
            "\n",
            "Epoch 00091: accuracy did not improve from 0.92678\n",
            "Epoch 92/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2040 - accuracy: 0.9255\n",
            "\n",
            "Epoch 00092: accuracy did not improve from 0.92678\n",
            "Epoch 93/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2010 - accuracy: 0.9288\n",
            "\n",
            "Epoch 00093: accuracy improved from 0.92678 to 0.92885, saving model to best_mnist.h5\n",
            "Epoch 94/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2035 - accuracy: 0.9256\n",
            "\n",
            "Epoch 00094: accuracy did not improve from 0.92885\n",
            "Epoch 95/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2025 - accuracy: 0.9279\n",
            "\n",
            "Epoch 00095: accuracy did not improve from 0.92885\n",
            "Epoch 96/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2061 - accuracy: 0.9271\n",
            "\n",
            "Epoch 00096: accuracy did not improve from 0.92885\n",
            "Epoch 97/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2010 - accuracy: 0.9270\n",
            "\n",
            "Epoch 00097: accuracy did not improve from 0.92885\n",
            "Epoch 98/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2005 - accuracy: 0.9289\n",
            "\n",
            "Epoch 00098: accuracy improved from 0.92885 to 0.92892, saving model to best_mnist.h5\n",
            "Epoch 99/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1994 - accuracy: 0.9278\n",
            "\n",
            "Epoch 00099: accuracy did not improve from 0.92892\n",
            "Epoch 100/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2014 - accuracy: 0.9268\n",
            "\n",
            "Epoch 00100: accuracy did not improve from 0.92892\n",
            "Epoch 101/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2013 - accuracy: 0.9284\n",
            "\n",
            "Epoch 00101: accuracy did not improve from 0.92892\n",
            "Epoch 102/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1989 - accuracy: 0.9277\n",
            "\n",
            "Epoch 00102: accuracy did not improve from 0.92892\n",
            "Epoch 103/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1973 - accuracy: 0.9293\n",
            "\n",
            "Epoch 00103: accuracy improved from 0.92892 to 0.92932, saving model to best_mnist.h5\n",
            "Epoch 104/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1993 - accuracy: 0.9287\n",
            "\n",
            "Epoch 00104: accuracy did not improve from 0.92932\n",
            "Epoch 105/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1988 - accuracy: 0.9285\n",
            "\n",
            "Epoch 00105: accuracy did not improve from 0.92932\n",
            "Epoch 106/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1996 - accuracy: 0.9279\n",
            "\n",
            "Epoch 00106: accuracy did not improve from 0.92932\n",
            "Epoch 107/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1968 - accuracy: 0.9289\n",
            "\n",
            "Epoch 00107: accuracy did not improve from 0.92932\n",
            "Epoch 108/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1966 - accuracy: 0.9298\n",
            "\n",
            "Epoch 00108: accuracy improved from 0.92932 to 0.92975, saving model to best_mnist.h5\n",
            "Epoch 109/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1959 - accuracy: 0.9310\n",
            "\n",
            "Epoch 00109: accuracy improved from 0.92975 to 0.93100, saving model to best_mnist.h5\n",
            "Epoch 110/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1922 - accuracy: 0.9310\n",
            "\n",
            "Epoch 00110: accuracy did not improve from 0.93100\n",
            "Epoch 111/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1935 - accuracy: 0.9302\n",
            "\n",
            "Epoch 00111: accuracy did not improve from 0.93100\n",
            "Epoch 112/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1938 - accuracy: 0.9305\n",
            "\n",
            "Epoch 00112: accuracy did not improve from 0.93100\n",
            "Epoch 113/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1945 - accuracy: 0.9314\n",
            "\n",
            "Epoch 00113: accuracy improved from 0.93100 to 0.93137, saving model to best_mnist.h5\n",
            "Epoch 114/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1922 - accuracy: 0.9314\n",
            "\n",
            "Epoch 00114: accuracy improved from 0.93137 to 0.93143, saving model to best_mnist.h5\n",
            "Epoch 115/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1930 - accuracy: 0.9309\n",
            "\n",
            "Epoch 00115: accuracy did not improve from 0.93143\n",
            "Epoch 116/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1921 - accuracy: 0.9298\n",
            "\n",
            "Epoch 00116: accuracy did not improve from 0.93143\n",
            "Epoch 117/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1918 - accuracy: 0.9326\n",
            "\n",
            "Epoch 00117: accuracy improved from 0.93143 to 0.93262, saving model to best_mnist.h5\n",
            "Epoch 118/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1929 - accuracy: 0.9304\n",
            "\n",
            "Epoch 00118: accuracy did not improve from 0.93262\n",
            "Epoch 119/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1913 - accuracy: 0.9317\n",
            "\n",
            "Epoch 00119: accuracy did not improve from 0.93262\n",
            "Epoch 120/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1910 - accuracy: 0.9307\n",
            "\n",
            "Epoch 00120: accuracy did not improve from 0.93262\n",
            "Epoch 121/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1892 - accuracy: 0.9325\n",
            "\n",
            "Epoch 00121: accuracy did not improve from 0.93262\n",
            "Epoch 122/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1893 - accuracy: 0.9318\n",
            "\n",
            "Epoch 00122: accuracy did not improve from 0.93262\n",
            "Epoch 123/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1913 - accuracy: 0.9316\n",
            "\n",
            "Epoch 00123: accuracy did not improve from 0.93262\n",
            "Epoch 124/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1875 - accuracy: 0.9333\n",
            "\n",
            "Epoch 00124: accuracy improved from 0.93262 to 0.93328, saving model to best_mnist.h5\n",
            "Epoch 125/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1868 - accuracy: 0.9326\n",
            "\n",
            "Epoch 00125: accuracy did not improve from 0.93328\n",
            "Epoch 126/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1882 - accuracy: 0.9317\n",
            "\n",
            "Epoch 00126: accuracy did not improve from 0.93328\n",
            "Epoch 127/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1868 - accuracy: 0.9323\n",
            "\n",
            "Epoch 00127: accuracy did not improve from 0.93328\n",
            "Epoch 128/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1866 - accuracy: 0.9341\n",
            "\n",
            "Epoch 00128: accuracy improved from 0.93328 to 0.93405, saving model to best_mnist.h5\n",
            "Epoch 129/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1842 - accuracy: 0.9339\n",
            "\n",
            "Epoch 00129: accuracy did not improve from 0.93405\n",
            "Epoch 130/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1864 - accuracy: 0.9337\n",
            "\n",
            "Epoch 00130: accuracy did not improve from 0.93405\n",
            "Epoch 131/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1836 - accuracy: 0.9348\n",
            "\n",
            "Epoch 00131: accuracy improved from 0.93405 to 0.93483, saving model to best_mnist.h5\n",
            "Epoch 132/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1839 - accuracy: 0.9334\n",
            "\n",
            "Epoch 00132: accuracy did not improve from 0.93483\n",
            "Epoch 133/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1859 - accuracy: 0.9329\n",
            "\n",
            "Epoch 00133: accuracy did not improve from 0.93483\n",
            "Epoch 134/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1846 - accuracy: 0.9345\n",
            "\n",
            "Epoch 00134: accuracy did not improve from 0.93483\n",
            "Epoch 135/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1836 - accuracy: 0.9343\n",
            "\n",
            "Epoch 00135: accuracy did not improve from 0.93483\n",
            "Epoch 136/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1857 - accuracy: 0.9340\n",
            "\n",
            "Epoch 00136: accuracy did not improve from 0.93483\n",
            "Epoch 137/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1819 - accuracy: 0.9349\n",
            "\n",
            "Epoch 00137: accuracy improved from 0.93483 to 0.93488, saving model to best_mnist.h5\n",
            "Epoch 138/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1838 - accuracy: 0.9337\n",
            "\n",
            "Epoch 00138: accuracy did not improve from 0.93488\n",
            "Epoch 139/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1804 - accuracy: 0.9349\n",
            "\n",
            "Epoch 00139: accuracy improved from 0.93488 to 0.93490, saving model to best_mnist.h5\n",
            "Epoch 140/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1817 - accuracy: 0.9348\n",
            "\n",
            "Epoch 00140: accuracy did not improve from 0.93490\n",
            "Epoch 141/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1818 - accuracy: 0.9359\n",
            "\n",
            "Epoch 00141: accuracy improved from 0.93490 to 0.93587, saving model to best_mnist.h5\n",
            "Epoch 142/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1834 - accuracy: 0.9335\n",
            "\n",
            "Epoch 00142: accuracy did not improve from 0.93587\n",
            "Epoch 143/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1797 - accuracy: 0.9359\n",
            "\n",
            "Epoch 00143: accuracy improved from 0.93587 to 0.93595, saving model to best_mnist.h5\n",
            "Epoch 144/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1787 - accuracy: 0.9359\n",
            "\n",
            "Epoch 00144: accuracy did not improve from 0.93595\n",
            "Epoch 145/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1805 - accuracy: 0.9353\n",
            "\n",
            "Epoch 00145: accuracy did not improve from 0.93595\n",
            "Epoch 146/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1790 - accuracy: 0.9360\n",
            "\n",
            "Epoch 00146: accuracy improved from 0.93595 to 0.93600, saving model to best_mnist.h5\n",
            "Epoch 147/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1802 - accuracy: 0.9363\n",
            "\n",
            "Epoch 00147: accuracy improved from 0.93600 to 0.93633, saving model to best_mnist.h5\n",
            "Epoch 148/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1797 - accuracy: 0.9364\n",
            "\n",
            "Epoch 00148: accuracy improved from 0.93633 to 0.93640, saving model to best_mnist.h5\n",
            "Epoch 149/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1782 - accuracy: 0.9369\n",
            "\n",
            "Epoch 00149: accuracy improved from 0.93640 to 0.93690, saving model to best_mnist.h5\n",
            "Epoch 150/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1773 - accuracy: 0.9367\n",
            "\n",
            "Epoch 00150: accuracy did not improve from 0.93690\n",
            "Epoch 151/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1765 - accuracy: 0.9378\n",
            "\n",
            "Epoch 00151: accuracy improved from 0.93690 to 0.93785, saving model to best_mnist.h5\n",
            "Epoch 152/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1777 - accuracy: 0.9369\n",
            "\n",
            "Epoch 00152: accuracy did not improve from 0.93785\n",
            "Epoch 153/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1746 - accuracy: 0.9378\n",
            "\n",
            "Epoch 00153: accuracy did not improve from 0.93785\n",
            "Epoch 154/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1770 - accuracy: 0.9377\n",
            "\n",
            "Epoch 00154: accuracy did not improve from 0.93785\n",
            "Epoch 155/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1747 - accuracy: 0.9376\n",
            "\n",
            "Epoch 00155: accuracy did not improve from 0.93785\n",
            "Epoch 156/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1753 - accuracy: 0.9381\n",
            "\n",
            "Epoch 00156: accuracy improved from 0.93785 to 0.93812, saving model to best_mnist.h5\n",
            "Epoch 157/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1763 - accuracy: 0.9370\n",
            "\n",
            "Epoch 00157: accuracy did not improve from 0.93812\n",
            "Epoch 158/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1747 - accuracy: 0.9382\n",
            "\n",
            "Epoch 00158: accuracy improved from 0.93812 to 0.93822, saving model to best_mnist.h5\n",
            "Epoch 159/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1770 - accuracy: 0.9368\n",
            "\n",
            "Epoch 00159: accuracy did not improve from 0.93822\n",
            "Epoch 160/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1734 - accuracy: 0.9375\n",
            "\n",
            "Epoch 00160: accuracy did not improve from 0.93822\n",
            "Epoch 161/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1733 - accuracy: 0.9393\n",
            "\n",
            "Epoch 00161: accuracy improved from 0.93822 to 0.93927, saving model to best_mnist.h5\n",
            "Epoch 162/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1762 - accuracy: 0.9372\n",
            "\n",
            "Epoch 00162: accuracy did not improve from 0.93927\n",
            "Epoch 163/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1724 - accuracy: 0.9386\n",
            "\n",
            "Epoch 00163: accuracy did not improve from 0.93927\n",
            "Epoch 164/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1700 - accuracy: 0.9398\n",
            "\n",
            "Epoch 00164: accuracy improved from 0.93927 to 0.93983, saving model to best_mnist.h5\n",
            "Epoch 165/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1732 - accuracy: 0.9386\n",
            "\n",
            "Epoch 00165: accuracy did not improve from 0.93983\n",
            "Epoch 166/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1718 - accuracy: 0.9385\n",
            "\n",
            "Epoch 00166: accuracy did not improve from 0.93983\n",
            "Epoch 167/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1734 - accuracy: 0.9384\n",
            "\n",
            "Epoch 00167: accuracy did not improve from 0.93983\n",
            "Epoch 168/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1700 - accuracy: 0.9394\n",
            "\n",
            "Epoch 00168: accuracy did not improve from 0.93983\n",
            "Epoch 169/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1718 - accuracy: 0.9397\n",
            "\n",
            "Epoch 00169: accuracy did not improve from 0.93983\n",
            "Epoch 170/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1725 - accuracy: 0.9397\n",
            "\n",
            "Epoch 00170: accuracy did not improve from 0.93983\n",
            "Epoch 171/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1681 - accuracy: 0.9395\n",
            "\n",
            "Epoch 00171: accuracy did not improve from 0.93983\n",
            "Epoch 172/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1685 - accuracy: 0.9396\n",
            "\n",
            "Epoch 00172: accuracy did not improve from 0.93983\n",
            "Epoch 173/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1710 - accuracy: 0.9403\n",
            "\n",
            "Epoch 00173: accuracy improved from 0.93983 to 0.94032, saving model to best_mnist.h5\n",
            "Epoch 174/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1708 - accuracy: 0.9386\n",
            "\n",
            "Epoch 00174: accuracy did not improve from 0.94032\n",
            "Epoch 175/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1711 - accuracy: 0.9379\n",
            "\n",
            "Epoch 00175: accuracy did not improve from 0.94032\n",
            "Epoch 176/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1684 - accuracy: 0.9394\n",
            "\n",
            "Epoch 00176: accuracy did not improve from 0.94032\n",
            "Epoch 177/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1682 - accuracy: 0.9405\n",
            "\n",
            "Epoch 00177: accuracy improved from 0.94032 to 0.94050, saving model to best_mnist.h5\n",
            "Epoch 178/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1703 - accuracy: 0.9394\n",
            "\n",
            "Epoch 00178: accuracy did not improve from 0.94050\n",
            "Epoch 179/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1676 - accuracy: 0.9403\n",
            "\n",
            "Epoch 00179: accuracy did not improve from 0.94050\n",
            "Epoch 180/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1691 - accuracy: 0.9387\n",
            "\n",
            "Epoch 00180: accuracy did not improve from 0.94050\n",
            "Epoch 181/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1668 - accuracy: 0.9404\n",
            "\n",
            "Epoch 00181: accuracy did not improve from 0.94050\n",
            "Epoch 182/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1672 - accuracy: 0.9398\n",
            "\n",
            "Epoch 00182: accuracy did not improve from 0.94050\n",
            "Epoch 183/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1679 - accuracy: 0.9401\n",
            "\n",
            "Epoch 00183: accuracy did not improve from 0.94050\n",
            "Epoch 184/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1643 - accuracy: 0.9413\n",
            "\n",
            "Epoch 00184: accuracy improved from 0.94050 to 0.94125, saving model to best_mnist.h5\n",
            "Epoch 185/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1672 - accuracy: 0.9404\n",
            "\n",
            "Epoch 00185: accuracy did not improve from 0.94125\n",
            "Epoch 186/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1646 - accuracy: 0.9419\n",
            "\n",
            "Epoch 00186: accuracy improved from 0.94125 to 0.94192, saving model to best_mnist.h5\n",
            "Epoch 187/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1647 - accuracy: 0.9414\n",
            "\n",
            "Epoch 00187: accuracy did not improve from 0.94192\n",
            "Epoch 188/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1651 - accuracy: 0.9413\n",
            "\n",
            "Epoch 00188: accuracy did not improve from 0.94192\n",
            "Epoch 189/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1691 - accuracy: 0.9407\n",
            "\n",
            "Epoch 00189: accuracy did not improve from 0.94192\n",
            "Epoch 190/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1638 - accuracy: 0.9409\n",
            "\n",
            "Epoch 00190: accuracy did not improve from 0.94192\n",
            "Epoch 191/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1643 - accuracy: 0.9424\n",
            "\n",
            "Epoch 00191: accuracy improved from 0.94192 to 0.94238, saving model to best_mnist.h5\n",
            "Epoch 192/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1632 - accuracy: 0.9422\n",
            "\n",
            "Epoch 00192: accuracy did not improve from 0.94238\n",
            "Epoch 193/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1640 - accuracy: 0.9410\n",
            "\n",
            "Epoch 00193: accuracy did not improve from 0.94238\n",
            "Epoch 194/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1649 - accuracy: 0.9410\n",
            "\n",
            "Epoch 00194: accuracy did not improve from 0.94238\n",
            "Epoch 195/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1621 - accuracy: 0.9415\n",
            "\n",
            "Epoch 00195: accuracy did not improve from 0.94238\n",
            "Epoch 196/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1614 - accuracy: 0.9413\n",
            "\n",
            "Epoch 00196: accuracy did not improve from 0.94238\n",
            "Epoch 197/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1620 - accuracy: 0.9422\n",
            "\n",
            "Epoch 00197: accuracy did not improve from 0.94238\n",
            "Epoch 198/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1642 - accuracy: 0.9427\n",
            "\n",
            "Epoch 00198: accuracy improved from 0.94238 to 0.94275, saving model to best_mnist.h5\n",
            "Epoch 199/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1620 - accuracy: 0.9430\n",
            "\n",
            "Epoch 00199: accuracy improved from 0.94275 to 0.94297, saving model to best_mnist.h5\n",
            "Epoch 200/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1603 - accuracy: 0.9433\n",
            "\n",
            "Epoch 00200: accuracy improved from 0.94297 to 0.94333, saving model to best_mnist.h5\n",
            "Epoch 201/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1608 - accuracy: 0.9437\n",
            "\n",
            "Epoch 00201: accuracy improved from 0.94333 to 0.94368, saving model to best_mnist.h5\n",
            "Epoch 202/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1620 - accuracy: 0.9435\n",
            "\n",
            "Epoch 00202: accuracy did not improve from 0.94368\n",
            "Epoch 203/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1610 - accuracy: 0.9428\n",
            "\n",
            "Epoch 00203: accuracy did not improve from 0.94368\n",
            "Epoch 204/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1605 - accuracy: 0.9427\n",
            "\n",
            "Epoch 00204: accuracy did not improve from 0.94368\n",
            "Epoch 205/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1607 - accuracy: 0.9419\n",
            "\n",
            "Epoch 00205: accuracy did not improve from 0.94368\n",
            "Epoch 206/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1615 - accuracy: 0.9422\n",
            "\n",
            "Epoch 00206: accuracy did not improve from 0.94368\n",
            "Epoch 207/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1604 - accuracy: 0.9425\n",
            "\n",
            "Epoch 00207: accuracy did not improve from 0.94368\n",
            "Epoch 208/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1569 - accuracy: 0.9446\n",
            "\n",
            "Epoch 00208: accuracy improved from 0.94368 to 0.94465, saving model to best_mnist.h5\n",
            "Epoch 209/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1575 - accuracy: 0.9434\n",
            "\n",
            "Epoch 00209: accuracy did not improve from 0.94465\n",
            "Epoch 210/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1585 - accuracy: 0.9441\n",
            "\n",
            "Epoch 00210: accuracy did not improve from 0.94465\n",
            "Epoch 211/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1604 - accuracy: 0.9448\n",
            "\n",
            "Epoch 00211: accuracy improved from 0.94465 to 0.94482, saving model to best_mnist.h5\n",
            "Epoch 212/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1565 - accuracy: 0.9448\n",
            "\n",
            "Epoch 00212: accuracy did not improve from 0.94482\n",
            "Epoch 213/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1576 - accuracy: 0.9441\n",
            "\n",
            "Epoch 00213: accuracy did not improve from 0.94482\n",
            "Epoch 214/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1604 - accuracy: 0.9421\n",
            "\n",
            "Epoch 00214: accuracy did not improve from 0.94482\n",
            "Epoch 215/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1566 - accuracy: 0.9438\n",
            "\n",
            "Epoch 00215: accuracy did not improve from 0.94482\n",
            "Epoch 216/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1557 - accuracy: 0.9444\n",
            "\n",
            "Epoch 00216: accuracy did not improve from 0.94482\n",
            "Epoch 217/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1580 - accuracy: 0.9436\n",
            "\n",
            "Epoch 00217: accuracy did not improve from 0.94482\n",
            "Epoch 218/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1565 - accuracy: 0.9445\n",
            "\n",
            "Epoch 00218: accuracy did not improve from 0.94482\n",
            "Epoch 219/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1577 - accuracy: 0.9447\n",
            "\n",
            "Epoch 00219: accuracy did not improve from 0.94482\n",
            "Epoch 220/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1587 - accuracy: 0.9426\n",
            "\n",
            "Epoch 00220: accuracy did not improve from 0.94482\n",
            "Epoch 221/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1590 - accuracy: 0.9435\n",
            "\n",
            "Epoch 00221: accuracy did not improve from 0.94482\n",
            "Epoch 222/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1559 - accuracy: 0.9446\n",
            "\n",
            "Epoch 00222: accuracy did not improve from 0.94482\n",
            "Epoch 223/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1551 - accuracy: 0.9454\n",
            "\n",
            "Epoch 00223: accuracy improved from 0.94482 to 0.94537, saving model to best_mnist.h5\n",
            "Epoch 224/300\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1562 - accuracy: 0.9442\n",
            "\n",
            "Epoch 00224: accuracy did not improve from 0.94537\n",
            "Epoch 225/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1546 - accuracy: 0.9456\n",
            "\n",
            "Epoch 00225: accuracy improved from 0.94537 to 0.94562, saving model to best_mnist.h5\n",
            "Epoch 226/300\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1551 - accuracy: 0.9454\n",
            "\n",
            "Epoch 00226: accuracy did not improve from 0.94562\n",
            "Epoch 227/300\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1565 - accuracy: 0.9452\n",
            "\n",
            "Epoch 00227: accuracy did not improve from 0.94562\n",
            "Epoch 228/300\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1558 - accuracy: 0.9452\n",
            "\n",
            "Epoch 00228: accuracy did not improve from 0.94562\n",
            "Epoch 229/300\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1548 - accuracy: 0.9452\n",
            "\n",
            "Epoch 00229: accuracy did not improve from 0.94562\n",
            "Epoch 230/300\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1561 - accuracy: 0.9451\n",
            "\n",
            "Epoch 00230: accuracy did not improve from 0.94562\n",
            "Epoch 231/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1531 - accuracy: 0.9457\n",
            "\n",
            "Epoch 00231: accuracy improved from 0.94562 to 0.94573, saving model to best_mnist.h5\n",
            "Epoch 232/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1546 - accuracy: 0.9466\n",
            "\n",
            "Epoch 00232: accuracy improved from 0.94573 to 0.94662, saving model to best_mnist.h5\n",
            "Epoch 233/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1537 - accuracy: 0.9460\n",
            "\n",
            "Epoch 00233: accuracy did not improve from 0.94662\n",
            "Epoch 234/300\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1512 - accuracy: 0.9455\n",
            "\n",
            "Epoch 00234: accuracy did not improve from 0.94662\n",
            "Epoch 235/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1538 - accuracy: 0.9459\n",
            "\n",
            "Epoch 00235: accuracy did not improve from 0.94662\n",
            "Epoch 236/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1501 - accuracy: 0.9465\n",
            "\n",
            "Epoch 00236: accuracy did not improve from 0.94662\n",
            "Epoch 237/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1520 - accuracy: 0.9466\n",
            "\n",
            "Epoch 00237: accuracy did not improve from 0.94662\n",
            "Epoch 238/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1519 - accuracy: 0.9456\n",
            "\n",
            "Epoch 00238: accuracy did not improve from 0.94662\n",
            "Epoch 239/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1548 - accuracy: 0.9440\n",
            "\n",
            "Epoch 00239: accuracy did not improve from 0.94662\n",
            "Epoch 240/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1531 - accuracy: 0.9446\n",
            "\n",
            "Epoch 00240: accuracy did not improve from 0.94662\n",
            "Epoch 241/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1513 - accuracy: 0.9453\n",
            "\n",
            "Epoch 00241: accuracy did not improve from 0.94662\n",
            "Epoch 242/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1489 - accuracy: 0.9472\n",
            "\n",
            "Epoch 00242: accuracy improved from 0.94662 to 0.94717, saving model to best_mnist.h5\n",
            "Epoch 243/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1509 - accuracy: 0.9467\n",
            "\n",
            "Epoch 00243: accuracy did not improve from 0.94717\n",
            "Epoch 244/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1511 - accuracy: 0.9460\n",
            "\n",
            "Epoch 00244: accuracy did not improve from 0.94717\n",
            "Epoch 245/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1507 - accuracy: 0.9464\n",
            "\n",
            "Epoch 00245: accuracy did not improve from 0.94717\n",
            "Epoch 246/300\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1513 - accuracy: 0.9453\n",
            "\n",
            "Epoch 00246: accuracy did not improve from 0.94717\n",
            "Epoch 247/300\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1499 - accuracy: 0.9463\n",
            "\n",
            "Epoch 00247: accuracy did not improve from 0.94717\n",
            "Epoch 248/300\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1474 - accuracy: 0.9466\n",
            "\n",
            "Epoch 00248: accuracy did not improve from 0.94717\n",
            "Epoch 249/300\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1479 - accuracy: 0.9482\n",
            "\n",
            "Epoch 00249: accuracy improved from 0.94717 to 0.94817, saving model to best_mnist.h5\n",
            "Epoch 250/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1509 - accuracy: 0.9469\n",
            "\n",
            "Epoch 00250: accuracy did not improve from 0.94817\n",
            "Epoch 251/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1502 - accuracy: 0.9477\n",
            "\n",
            "Epoch 00251: accuracy did not improve from 0.94817\n",
            "Epoch 252/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1515 - accuracy: 0.9469\n",
            "\n",
            "Epoch 00252: accuracy did not improve from 0.94817\n",
            "Epoch 253/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1515 - accuracy: 0.9471\n",
            "\n",
            "Epoch 00253: accuracy did not improve from 0.94817\n",
            "Epoch 254/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1505 - accuracy: 0.9473\n",
            "\n",
            "Epoch 00254: accuracy did not improve from 0.94817\n",
            "Epoch 255/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1510 - accuracy: 0.9459\n",
            "\n",
            "Epoch 00255: accuracy did not improve from 0.94817\n",
            "Epoch 256/300\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1475 - accuracy: 0.9482\n",
            "\n",
            "Epoch 00256: accuracy improved from 0.94817 to 0.94820, saving model to best_mnist.h5\n",
            "Epoch 257/300\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1478 - accuracy: 0.9475\n",
            "\n",
            "Epoch 00257: accuracy did not improve from 0.94820\n",
            "Epoch 258/300\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1472 - accuracy: 0.9480\n",
            "\n",
            "Epoch 00258: accuracy did not improve from 0.94820\n",
            "Epoch 259/300\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1455 - accuracy: 0.9479\n",
            "\n",
            "Epoch 00259: accuracy did not improve from 0.94820\n",
            "Epoch 260/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1452 - accuracy: 0.9477\n",
            "\n",
            "Epoch 00260: accuracy did not improve from 0.94820\n",
            "Epoch 261/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1446 - accuracy: 0.9492\n",
            "\n",
            "Epoch 00261: accuracy improved from 0.94820 to 0.94918, saving model to best_mnist.h5\n",
            "Epoch 262/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1487 - accuracy: 0.9468\n",
            "\n",
            "Epoch 00262: accuracy did not improve from 0.94918\n",
            "Epoch 263/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1452 - accuracy: 0.9491\n",
            "\n",
            "Epoch 00263: accuracy did not improve from 0.94918\n",
            "Epoch 264/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1468 - accuracy: 0.9486\n",
            "\n",
            "Epoch 00264: accuracy did not improve from 0.94918\n",
            "Epoch 265/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1493 - accuracy: 0.9468\n",
            "\n",
            "Epoch 00265: accuracy did not improve from 0.94918\n",
            "Epoch 266/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1476 - accuracy: 0.9483\n",
            "\n",
            "Epoch 00266: accuracy did not improve from 0.94918\n",
            "Epoch 267/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1453 - accuracy: 0.9475\n",
            "\n",
            "Epoch 00267: accuracy did not improve from 0.94918\n",
            "Epoch 268/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1456 - accuracy: 0.9487\n",
            "\n",
            "Epoch 00268: accuracy did not improve from 0.94918\n",
            "Epoch 269/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1466 - accuracy: 0.9477\n",
            "\n",
            "Epoch 00269: accuracy did not improve from 0.94918\n",
            "Epoch 270/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1450 - accuracy: 0.9488\n",
            "\n",
            "Epoch 00270: accuracy did not improve from 0.94918\n",
            "Epoch 271/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1431 - accuracy: 0.9495\n",
            "\n",
            "Epoch 00271: accuracy improved from 0.94918 to 0.94948, saving model to best_mnist.h5\n",
            "Epoch 272/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1470 - accuracy: 0.9477\n",
            "\n",
            "Epoch 00272: accuracy did not improve from 0.94948\n",
            "Epoch 273/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1466 - accuracy: 0.9487\n",
            "\n",
            "Epoch 00273: accuracy did not improve from 0.94948\n",
            "Epoch 274/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1438 - accuracy: 0.9491\n",
            "\n",
            "Epoch 00274: accuracy did not improve from 0.94948\n",
            "Epoch 275/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1437 - accuracy: 0.9489\n",
            "\n",
            "Epoch 00275: accuracy did not improve from 0.94948\n",
            "Epoch 276/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1434 - accuracy: 0.9495\n",
            "\n",
            "Epoch 00276: accuracy improved from 0.94948 to 0.94950, saving model to best_mnist.h5\n",
            "Epoch 277/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1410 - accuracy: 0.9493\n",
            "\n",
            "Epoch 00277: accuracy did not improve from 0.94950\n",
            "Epoch 278/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1452 - accuracy: 0.9488\n",
            "\n",
            "Epoch 00278: accuracy did not improve from 0.94950\n",
            "Epoch 279/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1440 - accuracy: 0.9489\n",
            "\n",
            "Epoch 00279: accuracy did not improve from 0.94950\n",
            "Epoch 280/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1422 - accuracy: 0.9499\n",
            "\n",
            "Epoch 00280: accuracy improved from 0.94950 to 0.94993, saving model to best_mnist.h5\n",
            "Epoch 281/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1459 - accuracy: 0.9483\n",
            "\n",
            "Epoch 00281: accuracy did not improve from 0.94993\n",
            "Epoch 282/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1448 - accuracy: 0.9486\n",
            "\n",
            "Epoch 00282: accuracy did not improve from 0.94993\n",
            "Epoch 283/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1423 - accuracy: 0.9502\n",
            "\n",
            "Epoch 00283: accuracy improved from 0.94993 to 0.95020, saving model to best_mnist.h5\n",
            "Epoch 284/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1415 - accuracy: 0.9502\n",
            "\n",
            "Epoch 00284: accuracy did not improve from 0.95020\n",
            "Epoch 285/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1426 - accuracy: 0.9499\n",
            "\n",
            "Epoch 00285: accuracy did not improve from 0.95020\n",
            "Epoch 286/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1449 - accuracy: 0.9486\n",
            "\n",
            "Epoch 00286: accuracy did not improve from 0.95020\n",
            "Epoch 287/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1410 - accuracy: 0.9494\n",
            "\n",
            "Epoch 00287: accuracy did not improve from 0.95020\n",
            "Epoch 288/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1447 - accuracy: 0.9504\n",
            "\n",
            "Epoch 00288: accuracy improved from 0.95020 to 0.95037, saving model to best_mnist.h5\n",
            "Epoch 289/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1412 - accuracy: 0.9496\n",
            "\n",
            "Epoch 00289: accuracy did not improve from 0.95037\n",
            "Epoch 290/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1419 - accuracy: 0.9494\n",
            "\n",
            "Epoch 00290: accuracy did not improve from 0.95037\n",
            "Epoch 291/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1447 - accuracy: 0.9490\n",
            "\n",
            "Epoch 00291: accuracy did not improve from 0.95037\n",
            "Epoch 292/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1385 - accuracy: 0.9514\n",
            "\n",
            "Epoch 00292: accuracy improved from 0.95037 to 0.95138, saving model to best_mnist.h5\n",
            "Epoch 293/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1383 - accuracy: 0.9512\n",
            "\n",
            "Epoch 00293: accuracy did not improve from 0.95138\n",
            "Epoch 294/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1414 - accuracy: 0.9500\n",
            "\n",
            "Epoch 00294: accuracy did not improve from 0.95138\n",
            "Epoch 295/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1390 - accuracy: 0.9506\n",
            "\n",
            "Epoch 00295: accuracy did not improve from 0.95138\n",
            "Epoch 296/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1380 - accuracy: 0.9508\n",
            "\n",
            "Epoch 00296: accuracy did not improve from 0.95138\n",
            "Epoch 297/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1393 - accuracy: 0.9511\n",
            "\n",
            "Epoch 00297: accuracy did not improve from 0.95138\n",
            "Epoch 298/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1411 - accuracy: 0.9505\n",
            "\n",
            "Epoch 00298: accuracy did not improve from 0.95138\n",
            "Epoch 299/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1395 - accuracy: 0.9502\n",
            "\n",
            "Epoch 00299: accuracy did not improve from 0.95138\n",
            "Epoch 300/300\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1390 - accuracy: 0.9514\n",
            "\n",
            "Epoch 00300: accuracy improved from 0.95138 to 0.95140, saving model to best_mnist.h5\n",
            "CPU times: user 10min 59s, sys: 1min 25s, total: 12min 25s\n",
            "Wall time: 10min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UEqCY6_dCpN"
      },
      "source": [
        "### 6)Best Model & Model Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A6YNn8FdHx2",
        "outputId": "37dd7083-5a94-400e-cadf-3dbcb78e5abb"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 1764\n",
            "-rw-r--r-- 1 root root 1801352 Mar 19 08:50 best_mnist.h5\n",
            "drwxr-xr-x 1 root root    4096 Mar  5 14:37 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qchRSKHTdJAs",
        "outputId": "101e1780-9c14-49e1-f381-559a458b0498"
      },
      "source": [
        "loss, accuracy = mnist_new.evaluate(X_test, y_test)\n",
        "\n",
        "print('Loss = {:.5f}'.format(loss))\n",
        "print('Accuracy = {:.5f}'.format(accuracy))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3348 - accuracy: 0.8905\n",
            "Loss = 0.33478\n",
            "Accuracy = 0.89050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEfqqGh-dP4i",
        "outputId": "87ae94ce-5728-4b41-e114-4a4e6077b95f"
      },
      "source": [
        "#첫번째 데이터 Probability\n",
        "np.set_printoptions(suppress=True, precision=9)\n",
        "\n",
        "print(mnist_new.predict(X_test[:1,:]))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.000016554 0.000000016 0.000001582 0.000000103 0.00000039  0.013910547 0.000007116 0.02038856  0.000136902 0.96553826 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZwZGe7ndVnZ",
        "outputId": "aa86d94a-66f2-4820-a880-1c48e46a24eb"
      },
      "source": [
        "#첫번째 데이터 Class\n",
        "print(mnist_new.predict_classes(X_test[:1, :]))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9J5Fth1sD4q"
      },
      "source": [
        "본 내용은 2021/03/19(금)에 작성되었습니다."
      ]
    }
  ]
}